{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Below Code will help build the corpus by scraping the data out of the DBS Website pages. A list of website urls required to be scraped are listed in the uurls variable and beautifulsouplibrary is used to scrape the text data out of the provided urls."
      ],
      "metadata": {
        "id": "9VVtkv1WVNHa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJl2jE_NUozW",
        "outputId": "0a56790f-006e-4aee-f3de-691b1daac960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "FQciQDDRVwaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "urls_dbs = [\n",
        "    \"https://www.dbs.ie/dbs-staff\",\n",
        "    \"https://www.dbs.ie/about-dbs/academic-departments\",\n",
        "    \"https://www.dbs.ie/about-dbs/our-academic-team\"\n",
        "]\n",
        "\n",
        "headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "\n",
        "#in colab the file will be created in the default location. It will be changed in python code file to store it onto disk.\n",
        "scraped_data_file_path = \"scraped_dbs.txt\"\n",
        "\n",
        "# file in which the scraped data will be loaded\n",
        "# data from 'h1', 'h2', 'h3', 'p', 'li' html tags will be extracted below.\n",
        "with open(scraped_data_file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for url in urls_dbs:\n",
        "        print(f\"Scraping: {url}\")\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            for tag in soup.find_all(['h1', 'h2', 'h3', 'p', 'li']):\n",
        "                text = tag.get_text(strip=True)\n",
        "                if text:\n",
        "                    f.write(f\"{text}\\n\")\n",
        "        else:\n",
        "            print(f\"Failed to fetch {url}: {response.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oHuGpb9U02U",
        "outputId": "49c4e09f-32e6-42c8-ce0a-cb2871940b76"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: https://www.dbs.ie/dbs-staff\n",
            "Scraping: https://www.dbs.ie/about-dbs/academic-departments\n",
            "Failed to fetch https://www.dbs.ie/about-dbs/academic-departments: 404\n",
            "Scraping: https://www.dbs.ie/about-dbs/our-academic-team\n",
            "Failed to fetch https://www.dbs.ie/about-dbs/our-academic-team: 404\n"
          ]
        }
      ]
    }
  ]
}